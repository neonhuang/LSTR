{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_08.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 08\n",
        "> You should try to implement your own solution for matrix vector multiplication, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below Icon.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_08.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f93f297-be33-47a4-e7c9-4dcf538eea92"
      },
      "source": [
        "# clone the code repo,\n",
        "# !pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "# %load_ext nvcc_plugin\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpdmlx_w86\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e340991c-3b44-4b8c-e152-5af0c2fe7bc2"
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.4 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sat Jun  7 03:20:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Naive approach of matrix vector multiplication\n",
        "Try to optimize it, you can do much better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af6e9e2-6bb1-481e-b180-abb734e82ddf"
      },
      "source": [
        "%%writefile matrix_vector_multiplication.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define M 100\n",
        "#define N 100\n",
        "#define MAX_ERR 1e-4\n",
        "\n",
        "__global__ void matrix_vector_multiplication(float* vector_result, float *matrix_a, float *vector_b, int m_row, int n_col)\n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        "\n",
        "    // blockIdx.x => which row\n",
        "    // blockDim.x => row length\n",
        "    // threadIdx.x => which element in this row\n",
        "\n",
        "    // Unique tid which can index each single element in the matrix\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // the condiction logic make sure, we only do the calculation in the matrix space\n",
        "    int size_of_the_matrix = m_row*n_col;\n",
        "    if(tid<size_of_the_matrix)\n",
        "    {\n",
        "        temp[tid] = matrix_a[tid] * vector_b[threadIdx.x]; // sum\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // synchronize all threads\n",
        "\n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        int index = blockIdx.x * blockDim.x;\n",
        "        for (int i = index; i < index + blockDim.x ; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        vector_result[blockIdx.x] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    float *martix_a, *martix_b, *vector_result;\n",
        "    float *d_martix_a, *d_martix_b, *d_vector_result;\n",
        "\n",
        "    martix_a = (float*)malloc(sizeof(float) * (M * N));\n",
        "    martix_b = (float*)malloc(sizeof(float) * (N));\n",
        "    vector_result = (float*)malloc(sizeof(float) * (M));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int raw_num = 0; raw_num < M; raw_num++)\n",
        "    {\n",
        "        for(int col_num = 0; col_num < N; col_num++)\n",
        "        {\n",
        "            int index = raw_num*N+col_num;\n",
        "            martix_a[index] = raw_num*3.14f+col_num;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for(int col_num = 0; col_num < N; col_num++)\n",
        "    {\n",
        "        martix_b[col_num] = col_num+1;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_martix_a, sizeof(float) * (M * N));\n",
        "    cudaMalloc((void**)&d_martix_b, sizeof(float) * N);\n",
        "    cudaMalloc((void**)&d_vector_result, sizeof(float) * M);\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_martix_a, martix_a, sizeof(float) * (M * N), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_martix_b, martix_b, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "    matrix_vector_multiplication<<<M,N,sizeof(float) * (M * N)>>>(d_vector_result, d_martix_a, d_martix_b, M, N);\n",
        "\n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(vector_result, d_vector_result, sizeof(float) * M, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Test the result\n",
        "    for(int i = 0; i < M; i++)\n",
        "    {\n",
        "        float temp_sum =0;\n",
        "        for(int j = 0; j < N; j++)\n",
        "        {\n",
        "            int index = i*N+j;\n",
        "            temp_sum = temp_sum + martix_a[index]*martix_b[j];\n",
        "        }\n",
        "        //printf(\"out[%d]: %f, %f\\n\", i, temp_sum, vector_result[i]);\n",
        "\n",
        "        assert(fabs(vector_result[i] - temp_sum) < MAX_ERR);\n",
        "    }\n",
        "    printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_martix_a);\n",
        "    cudaFree(d_martix_b);\n",
        "    cudaFree(d_vector_result);\n",
        "    free(martix_a);\n",
        "    free(martix_b);\n",
        "    free(vector_result);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrix_vector_multiplication.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a660911-a1bc-47e2-da69-cc606d885c1d"
      },
      "source": [
        "!nvcc -o matrix_vector_multiplication matrix_vector_multiplication.cu\n",
        "!nvprof ./matrix_vector_multiplication 0 0\n",
        "!nvprof ./matrix_vector_multiplication 1 0\n",
        "!nvprof ./matrix_vector_multiplication 2 0\n",
        "!nvprof ./matrix_vector_multiplication 3 0\n",
        "!nvprof ./matrix_vector_multiplication 4 0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==560== NVPROF is profiling process 560, command: ./matrix_vector_multiplication 0 0\n",
            "matrix_vector_multiplication: matrix_vector_multiplication.cu:91: int main(): Assertion `fabs(vector_result[i] - temp_sum) < MAX_ERR' failed.\n",
            "==560== Profiling application: ./matrix_vector_multiplication 0 0\n",
            "==560== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==560== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==575== NVPROF is profiling process 575, command: ./matrix_vector_multiplication 1 0\n",
            "matrix_vector_multiplication: matrix_vector_multiplication.cu:91: int main(): Assertion `fabs(vector_result[i] - temp_sum) < MAX_ERR' failed.\n",
            "==575== Profiling application: ./matrix_vector_multiplication 1 0\n",
            "==575== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==575== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==586== NVPROF is profiling process 586, command: ./matrix_vector_multiplication 2 0\n",
            "matrix_vector_multiplication: matrix_vector_multiplication.cu:91: int main(): Assertion `fabs(vector_result[i] - temp_sum) < MAX_ERR' failed.\n",
            "==586== Profiling application: ./matrix_vector_multiplication 2 0\n",
            "==586== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==586== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==597== NVPROF is profiling process 597, command: ./matrix_vector_multiplication 3 0\n",
            "matrix_vector_multiplication: matrix_vector_multiplication.cu:91: int main(): Assertion `fabs(vector_result[i] - temp_sum) < MAX_ERR' failed.\n",
            "==597== Profiling application: ./matrix_vector_multiplication 3 0\n",
            "==597== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==597== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==612== NVPROF is profiling process 612, command: ./matrix_vector_multiplication 4 0\n",
            "matrix_vector_multiplication: matrix_vector_multiplication.cu:91: int main(): Assertion `fabs(vector_result[i] - temp_sum) < MAX_ERR' failed.\n",
            "==612== Profiling application: ./matrix_vector_multiplication 4 0\n",
            "==612== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==612== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n"
          ]
        }
      ]
    }
  ]
}