{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 07\n",
        "> You should try to implement your own solution for vector dot product, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below Icon.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_07.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx",
        "outputId": "b1953f82-d032-4974-9855-152ed0617844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# clone the code repo,\n",
        "# !pip install git+git://github.com/depctg/nvcc4jupyter.git\n",
        "# %load_ext nvcc_plugin\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp2ugj0fnd\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt",
        "outputId": "082a91f7-9f4c-49b6-c698-00f363eeea93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.4 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sat Jun  7 03:18:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Naive approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3",
        "outputId": "cb0d461d-a007-454d-bd87-2c85b3a5916c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length)\n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        "\n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    // so if threadIdx.x=0, and blockDim.x=10,\n",
        "    // then this thread is responsible for calculating temp[0], temp[10], temp[20]\n",
        "    // similiarly, the following thread will calculate temp[1], temp[11], temp[21]\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[i] = vector_a[i] * vector_b[i];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // synchronize all threads\n",
        "\n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < vertor_length; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        "\n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The argument supplied is %s\\n\", argv[1]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);\n",
        "\n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"One argument expected.\\n\");\n",
        "\n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * vector_length>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        "\n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "\n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unl0xR2C_27V"
      },
      "source": [
        "## Optimized approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba05ukJC8AKq",
        "outputId": "8a861d56-ee6f-4e62-e63e-367def6c5830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length)\n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        "\n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    temp[threadIdx.x] = 0;\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[threadIdx.x] = temp[threadIdx.x] + vector_a[i] * vector_b[i];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // synchronize all threads\n",
        "\n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        int thread_num = (vertor_length+blockDim.x)/blockDim.x;\n",
        "        for (int i = 0; i < thread_num; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        "\n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The arguments supplied are %s, %s\\n\", argv[1], argv[2]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);\n",
        "\n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"Two argument expected.\\n\");\n",
        "      return 0;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * thread_num>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        "\n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "\n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT",
        "outputId": "c3744bd2-14b4-4500-db91-f897a1230848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 0 0\n",
        "!nvprof ./exercise01 1 0\n",
        "!nvprof ./exercise01 2 0\n",
        "!nvprof ./exercise01 3 0\n",
        "!nvprof ./exercise01 4 0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1380== NVPROF is profiling process 1380, command: ./exercise01 0 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1380== Profiling application: ./exercise01 0 0\n",
            "==1380== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1380== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1395== NVPROF is profiling process 1395, command: ./exercise01 1 0\n",
            "result[0] = 0.000000\n",
            "free(): double free detected in tcache 2\n",
            "==1395== Profiling application: ./exercise01 1 0\n",
            "==1395== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1395== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1406== NVPROF is profiling process 1406, command: ./exercise01 2 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1406== Profiling application: ./exercise01 2 0\n",
            "==1406== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1406== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1421== NVPROF is profiling process 1421, command: ./exercise01 3 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1421== Profiling application: ./exercise01 3 0\n",
            "==1421== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1421== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1432== NVPROF is profiling process 1432, command: ./exercise01 4 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1432== Profiling application: ./exercise01 4 0\n",
            "==1432== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1432== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20hMfub0Tr2",
        "outputId": "c445cb5f-4083-4827-9465-d492449662a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 4 0\n",
        "!nvprof ./exercise01 4 1\n",
        "!nvprof ./exercise01 4 2\n",
        "!nvprof ./exercise01 4 3\n",
        "!nvprof ./exercise01 4 4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1510== NVPROF is profiling process 1510, command: ./exercise01 4 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1510== Profiling application: ./exercise01 4 0\n",
            "==1510== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1510== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1521== NVPROF is profiling process 1521, command: ./exercise01 4 1\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1521== Profiling application: ./exercise01 4 1\n",
            "==1521== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1521== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1532== NVPROF is profiling process 1532, command: ./exercise01 4 2\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1532== Profiling application: ./exercise01 4 2\n",
            "==1532== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1532== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1549== NVPROF is profiling process 1549, command: ./exercise01 4 3\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1549== Profiling application: ./exercise01 4 3\n",
            "==1549== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1549== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==1560== NVPROF is profiling process 1560, command: ./exercise01 4 4\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==1560== Profiling application: ./exercise01 4 4\n",
            "==1560== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==1560== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n"
          ]
        }
      ]
    }
  ]
}