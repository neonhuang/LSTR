{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_07.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JwSwNW9QmT"
      },
      "source": [
        "\n",
        "# CUDA Exercise 07\n",
        "> You should try to implement your own solution for vector dot product, and try to parallelize the computation.\n",
        "\n",
        "This Jupyter Notebook can also be open by the google colab, so you don't have to buy a PC with a graphic card to play with CUDA. To launch the Google Colab, please click the below Icon.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/CUDA_Learning/blob/main/Solution/Exercise_07.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOEai4hb95Ip"
      },
      "source": [
        "## Initialize the CUDA dev environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmwwI7H5nDx",
        "outputId": "80fc50f9-c22b-4b3b-be1b-4bee2ad02ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# clone the code repo,\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp_i7peyn4\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Zeyyo4_gNH"
      },
      "source": [
        "## Check the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PT4QpR6oxt"
      },
      "source": [
        "!lsb_release -a\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6KTYqE_n7H"
      },
      "source": [
        "## Naive approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev5_BW1z80S3",
        "outputId": "6f162203-6505-49d7-b4cd-4aa186acfe8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length)\n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        "\n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    // so if threadIdx.x=0, and blockDim.x=10,\n",
        "    // then this thread is responsible for calculating temp[0], temp[10], temp[20]\n",
        "    // similiarly, the following thread will calculate temp[1], temp[11], temp[21]\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[i] = vector_a[i] * vector_b[i];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // synchronize all threads\n",
        "\n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        for (int i = 0; i < vertor_length; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        "\n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The argument supplied is %s\\n\", argv[1]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);\n",
        "\n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"One argument expected.\\n\");\n",
        "\n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * vector_length>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        "\n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "\n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing exercise01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unl0xR2C_27V"
      },
      "source": [
        "## Optimized approach of vector dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba05ukJC8AKq",
        "outputId": "f539179a-0eef-477d-9567-9383d7573678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile exercise01.cu\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MAX_ERR 0.1\n",
        "#define MULTI_TIMES_RUN 1\n",
        "\n",
        "__global__ void vector_dot_product(float *result, float *vector_a, float *vector_b, int vertor_length)\n",
        "{\n",
        "    extern __shared__ float temp[];\n",
        "\n",
        "    int index = threadIdx.x;    // index offset of this thread\n",
        "    int stride = blockDim.x;    // stride step of each iteration\n",
        "\n",
        "    temp[threadIdx.x] = 0;\n",
        "    for(int i = index; i < vertor_length; i += stride)\n",
        "    {\n",
        "        temp[threadIdx.x] = temp[threadIdx.x] + vector_a[i] * vector_b[i];\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // synchronize all threads\n",
        "\n",
        "    // The accumulation only needs to happen at thread_0\n",
        "    if (threadIdx.x == 0)\n",
        "    {\n",
        "        float sum = 0;\n",
        "        int thread_num = (vertor_length+blockDim.x)/blockDim.x;\n",
        "        for (int i = 0; i < thread_num; i++)\n",
        "        {\n",
        "            sum += temp[i];\n",
        "        }\n",
        "        *result=sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    float *vector_a, *vector_b, *result;\n",
        "    float *d_vector_a, *d_vector_b, *d_result;\n",
        "    int list_of_thread_num[]={1,64,128,256,512,1024};\n",
        "    int list_of_vector_length[]={100,200,1000,2000,10000};\n",
        "    int thread_num = 1;\n",
        "    int vector_length = 1000;\n",
        "\n",
        "     if( argc == 3 ) {\n",
        "      //printf(\"The arguments supplied are %s, %s\\n\", argv[1], argv[2]);\n",
        "      int arg1 = atoi(argv[1]);  //argv[0] is the program name\n",
        "                                //atoi = ascii to int\n",
        "      int arg2 = atoi(argv[2]);\n",
        "\n",
        "      vector_length = list_of_vector_length[arg1];\n",
        "      thread_num = list_of_thread_num[arg2];\n",
        "    }\n",
        "    else if( argc > 2 ) {\n",
        "      printf(\"Too many arguments supplied.\\n\");\n",
        "    }\n",
        "    else {\n",
        "      printf(\"Two argument expected.\\n\");\n",
        "      return 0;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on CPU\n",
        "    vector_a = (float*)malloc(sizeof(float) * vector_length);\n",
        "    vector_b = (float*)malloc(sizeof(float) * vector_length);\n",
        "    result = (float*)malloc(sizeof(float));\n",
        "\n",
        "    // data initializtion\n",
        "    for(int i = 0; i < vector_length; i++)\n",
        "    {\n",
        "        vector_a[i] = 0.1f;\n",
        "        vector_b[i] = 0.9f;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on GPU\n",
        "    cudaMalloc((void**)&d_vector_a, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_vector_b, sizeof(float) * vector_length);\n",
        "    cudaMalloc((void**)&d_result, sizeof(float));\n",
        "\n",
        "    // copy operator to GPU\n",
        "    cudaMemcpy(d_vector_a, vector_a, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vector_b, vector_b, sizeof(float) * vector_length, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // GPU do the work, CPU waits\n",
        "#if MULTI_TIMES_RUN\n",
        "    for(int i=0; i< 10; i++)\n",
        "    {\n",
        "#endif\n",
        "        vector_dot_product<<<1,thread_num,sizeof(float) * thread_num>>>(d_result, d_vector_a, d_vector_b, vector_length);\n",
        "#if MULTI_TIMES_RUN\n",
        "    }\n",
        " #endif\n",
        "\n",
        "    // Get results from the GPU\n",
        "    cudaMemcpy(result, d_result, sizeof(float),\n",
        "               cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Test the result\n",
        "    //assert(fabs(*result - vector_length*2*3.14) < MAX_ERR);\n",
        "\n",
        "    // you only need them for checking if the math is correct\n",
        "     printf(\"result[0] = %f\\n\", result[0]);\n",
        "    // printf(\"PASSED\\n\");\n",
        "\n",
        "    // Free the memory\n",
        "    cudaFree(d_vector_a);\n",
        "    cudaFree(d_vector_b);\n",
        "    cudaFree(d_result);\n",
        "    free(vector_a);\n",
        "    free(vector_a);\n",
        "    free(result);\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting exercise01.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsEJesxACRz"
      },
      "source": [
        "## Evaluation to collect enough information for the benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjisNLsazjUT",
        "outputId": "7714ed7f-cac1-40e3-87c0-e09ea9566325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 0 0\n",
        "!nvprof ./exercise01 1 0\n",
        "!nvprof ./exercise01 2 0\n",
        "!nvprof ./exercise01 3 0\n",
        "!nvprof ./exercise01 4 0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2558== NVPROF is profiling process 2558, command: ./exercise01 0 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2558== Profiling application: ./exercise01 0 0\n",
            "==2558== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2558== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2573== NVPROF is profiling process 2573, command: ./exercise01 1 0\n",
            "result[0] = 0.000000\n",
            "free(): double free detected in tcache 2\n",
            "==2573== Profiling application: ./exercise01 1 0\n",
            "==2573== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2573== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2584== NVPROF is profiling process 2584, command: ./exercise01 2 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2584== Profiling application: ./exercise01 2 0\n",
            "==2584== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2584== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2595== NVPROF is profiling process 2595, command: ./exercise01 3 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2595== Profiling application: ./exercise01 3 0\n",
            "==2595== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2595== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2610== NVPROF is profiling process 2610, command: ./exercise01 4 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2610== Profiling application: ./exercise01 4 0\n",
            "==2610== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2610== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J20hMfub0Tr2",
        "outputId": "f6b0cb65-400d-4cd9-db23-ce960411b5c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc -o exercise01 exercise01.cu\n",
        "!nvprof ./exercise01 4 0\n",
        "!nvprof ./exercise01 4 1\n",
        "!nvprof ./exercise01 4 2\n",
        "!nvprof ./exercise01 4 3\n",
        "!nvprof ./exercise01 4 4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2666== NVPROF is profiling process 2666, command: ./exercise01 4 0\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2666== Profiling application: ./exercise01 4 0\n",
            "==2666== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2666== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2683== NVPROF is profiling process 2683, command: ./exercise01 4 1\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2683== Profiling application: ./exercise01 4 1\n",
            "==2683== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2683== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2694== NVPROF is profiling process 2694, command: ./exercise01 4 2\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2694== Profiling application: ./exercise01 4 2\n",
            "==2694== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2694== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2709== NVPROF is profiling process 2709, command: ./exercise01 4 3\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2709== Profiling application: ./exercise01 4 3\n",
            "==2709== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2709== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n",
            "==2720== NVPROF is profiling process 2720, command: ./exercise01 4 4\n",
            "result[0] = 0.000000\n",
            "double free or corruption (!prev)\n",
            "==2720== Profiling application: ./exercise01 4 4\n",
            "==2720== Profiling result:\n",
            "No kernels were profiled.\n",
            "No API activities were profiled.\n",
            "==2720== Warning: Some profiling data are not recorded.\n",
            "======== Error: Application received signal 134\n"
          ]
        }
      ]
    }
  ]
}